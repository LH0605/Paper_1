import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage.filters import gaussian_filter1d

# delete index: 5,7,9,11
epsilons = [0, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0] # linear loss

# delete index: 4,5,6,13,14
# epsilons = [0, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 1.0, 1.5, 2.0] # gaussian

# epsilons = [0, 1., 3., 4., 7., 8., 10., 12.] # poisson



test_losses= [[-3.66313959,-5.10366424,-5.90411216,-6.49421201,-7.09028725,-7.45653009
,-7.93564351,-8.17017362,-8.43033985,-8.69833788,-8.88696053,-8.90209677
,-9.07632468,-9.14236472,-9.14163819,-9.33032328,-9.33030802,-9.41770723
,-9.4359537,-9.47789087,-9.50314945,-9.54529568,-9.60935732,-9.60998319
,-9.57824752]
,[-3.53600482,-5.10775615,-6.07004685,-6.74799559,-6.97819215,-7.59122028
,-7.87555267,-8.01101226,-8.36182724,-8.58931369,-8.78129658,-8.89238667
,-8.86564706,-9.09879651,-9.14205215,-9.26320239,-9.28414425,-9.34771235
,-9.402775,-9.47178936,-9.39776202,-9.57748386,-9.57161666,-9.55889505
,-9.56643494]
,[-3.6318162,-5.04741251,-5.7484668,-6.36218556,-6.80736414,-7.37048288
,-7.62611668,-7.80758399,-8.15599123,-8.23183478,-8.28165207,-8.53900219
,-8.59505032,-8.76380944,-8.68019635,-8.86268843,-8.9878076,-9.10264162
,-9.15247349,-9.17164818,-9.14831924,-9.16768082,-9.27629668,-9.29733127
,-9.33513263]
,[-3.35397404,-4.92418345,-5.50580774,-5.976997,-6.33355269,-6.76457191
,-7.01871981,-7.11940874,-7.48256798,-7.40820556,-7.69438555,-7.75623299
,-7.88776007,-7.90817055,-7.9688851,-8.08255222,-8.25314226,-8.25872486
,-8.45693415,-8.42289454,-8.50431111,-8.55966785,-8.56776514,-8.62251227
,-8.67409048]
,[-3.4559088,-4.6308785,-5.26517999,-5.81538092,-6.16635164,-6.4614538
,-6.63086089,-6.78929871,-7.04261705,-7.17858038,-7.06819066,-7.26409726
,-7.37955039,-7.42062647,-7.51429297,-7.53876881,-7.68423124,-7.62906926
,-7.89043448,-7.98123121,-7.8616715,-8.03049545,-8.15851778,-8.11199006
,-8.26203432]
,[-3.52241974,-4.35091217,-5.43515421,-5.59546573,-6.03876855,-6.1125139
,-6.43824588,-6.53218362,-6.61392683,-6.81415415,-7.06953331,-7.05054813
,-7.03037082,-7.10446573,-7.27610123,-7.27477846,-7.50577422,-7.49404632
,-7.50047138,-7.64514953,-7.77649172,-7.65182318,-7.76076782,-7.80480956
,-7.97728253]
,[-3.5801038,-4.35728492,-5.04712073,-5.53217918,-5.84137577,-6.01064626
,-6.20897792,-6.41306168,-6.56340676,-6.58394172,-6.74777931,-6.78593214
,-6.84153525,-6.85126497,-7.0490247,-6.98506789,-7.00935229,-7.11436483
,-7.20257977,-7.36110786,-7.3046841,-7.36536017,-7.4101124,-7.51722584
,-7.47723839]
,[-3.42832815,-4.51473149,-5.04286339,-5.46254733,-5.5791912,-5.68784443
,-5.74755061,-6.18121862,-6.07441868,-6.27574145,-6.35941701,-6.36609108
,-6.55338664,-6.69486181,-6.68361789,-6.83255145,-6.91746395,-6.87962966
,-6.96498922,-6.894996,-7.11563853,-7.07167476,-7.10780936,-7.01080441
,-7.20746806]
,[-3.47626618,-4.36365613,-4.89968284,-5.0331305,-5.4928718,-5.74991547
,-5.68117733,-6.03372808,-6.09441137,-5.94826314,-6.01710101,-6.19600994
,-6.21197809,-6.31475637,-6.35733846,-6.35397083,-6.45314237,-6.39662799
,-6.56995526,-6.5400115,-6.70451181,-6.61297738,-6.6991117,-6.76200991
,-6.90036807]
,[-3.19319728,-4.10921666,-4.62438219,-5.00070653,-5.27787335,-5.41120489
,-5.37862317,-5.62455799,-5.69895692,-5.70123071,-5.84118023,-5.68600393
,-5.87873583,-5.87820282,-5.99484752,-6.14456513,-6.0431102,-6.18764469
,-6.14042717,-6.21103185,-6.26716616,-6.1720447,-6.35413359,-6.26653347
,-6.35882781]
,[-3.38732254,-4.13804503,-4.52654801,-4.9073683,-5.06302476,-5.16980911
,-5.26076654,-5.3807157,-5.28482649,-5.47914639,-5.4477485,-5.49861272
,-5.37837049,-5.78290607,-5.6280344,-5.71625363,-5.76996114,-5.707238
,-5.68619377,-5.94146611,-5.91671963,-5.86724559,-5.98432152,-5.98629627
,-6.01958602]
,[-3.23069416,-4.11277231,-4.38736305,-4.63174177,-4.77831563,-5.06534428
,-4.8550285,-5.02744346,-5.11072441,-5.18225931,-5.30711722,-5.22470087
,-5.30568568,-5.39511813,-5.30797701,-5.36573042,-5.45295308,-5.45425713
,-5.53739369,-5.60204845,-5.54684381,-5.51383601,-5.34431614,-5.48858405
,-5.54105194]
,[-3.24564049,-4.10281423,-4.51328797,-4.60907823,-4.76485196,-4.71552838
,-4.70537823,-4.83648298,-4.83133501,-4.94434925,-4.78651323,-4.89483174
,-5.02865179,-4.91333081,-4.80593284,-5.10173096,-5.04231043,-4.93192133
,-4.87572928,-4.98463695,-4.9470977,-5.1585562,-5.05284015,-5.12586055
,-5.20016352]
,[-2.92692843,-3.25436196,-3.74940222,-3.36289401,-3.61912889,-3.55591361
,-3.49356678,-3.2999228,-3.34622114,-3.20611518,-3.18680262,-3.07333269
,-3.06169962,-2.91107477,-3.13445087,-3.02174975,-3.02570914,-2.97431601
,-2.92896102,-3.02311325,-3.02569318,-2.96831679,-3.02317659,-3.09102098
,-3.08094605]
,[-2.73952541,-2.95389805,-3.05800346,-2.87554785,-2.75067253,-2.69317536
,-2.53016105,-2.48389775,-2.40443055,-2.18714649,-2.21764421,-2.21062425
,-2.08251824,-2.12395957,-2.03169925,-2.08037345,-1.96026991,-2.0892294
,-2.14600965,-2.05368952,-2.11174753,-2.0551741,-1.90948599,-2.00471352
,-2.18002982]
,[-2.58086553,-2.60300102,-2.44728368,-2.19346677,-2.25762509,-2.01636178
,-1.93176515,-1.71184788,-1.73959001,-1.6319249,-1.63332979,-1.5564444
,-1.49611022,-1.47164547,-1.62058661,-1.3695718,-1.41670713,-1.57200679
,-1.47703899,-1.47799349,-1.51391138,-1.56964384,-1.56204834,-1.5625022
,-1.6072426,]
,[-2.19110672,-2.07920528,-1.86714045,-1.55417536,-1.35978095,-1.31129551
,-1.12013035,-1.12242351,-1.13914274,-0.96008427,-0.93494226,-0.95381487
,-0.95597074,-0.96748784,-0.97618439,-1.02908712,-1.03396535,-1.0901858
,-1.13853673,-1.1551663,-1.21609968,-1.21573617,-1.29019185,-1.31712001
,-1.30880146]]
# print(len(test_losses))
del(test_losses[5])
del(test_losses[6])
del(test_losses[7])
del(test_losses[8])
# del(test_losses[10])

if len(epsilons) != len(test_losses):
    raise Exception("should be the same, check epsilons")
TRAIN_SIZE = len(test_losses[0])
train_sizes = np.arange(1, TRAIN_SIZE+1)

plt.title("Linear Loss 10D Gaussian with OPT")
plt.xlabel("Size of Training Dataset")
plt.ylabel("Test Loss")
for i in range(len(epsilons)): # range(len(epsilons)):
    print('eps:', epsilons[i])
    ysmoothed = gaussian_filter1d(test_losses[i], sigma=2)
    plt.plot(train_sizes, ysmoothed, label=f"Ɛ = {epsilons[i]}")
plt.legend(loc="right")
plt.savefig(f"linear_opt_10d_all.png")
plt.clf()

"""
# 1
plt.title("Linear Regression Gaussian")
plt.xlabel("Size of Training Dataset")
plt.ylabel("Test Loss")
for i in [1,2,3]: # range(len(epsilons)):
    print('eps:', epsilons[i])
    plt.plot(train_sizes, test_losses[i], label=f"Ɛ = {epsilons[i]}")
plt.legend(loc="best")
plt.savefig(f"linreg_gaussian_fgsm_1d_weak.png")
plt.clf()

# 2
for i in [13, 14, 15, 16]:
    print('eps:', epsilons[i])
    plt.plot(train_sizes, test_losses[i], label=f"Ɛ = {epsilons[i]}")
plt.legend(loc="best")
plt.savefig(f"linreg_gaussian_fgsm_1d_strong.png")
"""
